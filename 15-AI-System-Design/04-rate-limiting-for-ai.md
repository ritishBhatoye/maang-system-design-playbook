# ðŸš¦ Rate Limiting for AI (2026 MAANG Edition)

## 1. Why This Matters in Interviews

AI rate limiting is **fundamentally different** from traditional API rate limiting. Interviewers test this to see if you understand token-based limits, cost protection, and fair usage.

---

## 2. Problem Interviewers Are Testing

- Do you understand token-based rate limiting?
- Can you protect against cost attacks?
- How do you handle bursty AI workloads?
- Can you implement fair queuing for expensive operations?

---

## 3. Key Concepts

### Why AI Rate Limiting is Different

```
Traditional API:
â”œâ”€â”€ Rate limit: 100 requests/minute
â”œâ”€â”€ Cost per request: ~$0.0001
â”œâ”€â”€ All requests roughly equal

AI/LLM API:
â”œâ”€â”€ Rate limit: 100K tokens/minute
â”œâ”€â”€ Cost per request: $0.001 - $1.00 (1000x variance!)
â”œâ”€â”€ Request "size" varies massively
â””â”€â”€ Single abusive request can cost $50+
```

### What to Limit

| Dimension | Why | Limit Example |
|-----------|-----|---------------|
| **Requests/min** | Prevent abuse | 60/min per user |
| **Tokens/min** | Control cost | 100K tokens/min |
| **Concurrent** | Manage capacity | 5 concurrent/user |
| **Daily spend** | Cost protection | $50/day per user |
| **Context size** | Memory protection | 100K tokens max |

---

## 4. Rate Limiting Strategies

### 1. Token Bucket (Bursty-Friendly)

```
Bucket: 100K tokens
Refill: 10K tokens/minute

Request needs 5K tokens:
â”œâ”€â”€ Tokens available? â†’ Consume, proceed
â”œâ”€â”€ Not enough? â†’ Wait or reject (429)

Benefits:
â”œâ”€â”€ Allows bursts up to bucket size
â”œâ”€â”€ Smoothly limits sustained rate
â””â”€â”€ Fair for varied request sizes
```

### 2. Sliding Window

```
Track usage in rolling window:

[â”€â”€â”€â”€â”€â”€â”€â”€ 1 minute window â”€â”€â”€â”€â”€â”€â”€â”€]
   â”‚5Kâ”‚ â”‚10Kâ”‚ â”‚3Kâ”‚ â”‚8Kâ”‚    â”‚?â”‚
                              â†‘
                        New request: 15K tokens
                        Current usage: 26K
                        Limit: 100K
                        â†’ Allowed (26K + 15K < 100K)
```

### 3. Cost-Based Limiting

```python
def check_rate_limit(user_id, request_tokens):
    # Check multiple dimensions
    checks = [
        rate_limiter.check("requests", user_id, 1),
        rate_limiter.check("tokens", user_id, request_tokens),
        rate_limiter.check("daily_spend", user_id, estimate_cost(request_tokens)),
    ]
    
    if not all(checks):
        return 429, "Rate limit exceeded"
    
    return 200, "OK"
```

---

## 5. Architecture Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Request                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Pre-Request Checks                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚Request/min  â”‚ â”‚ Token/min   â”‚ â”‚ Daily spend budget     â”‚ â”‚
â”‚  â”‚(60 limit)   â”‚ â”‚(100K limit) â”‚ â”‚($50 limit)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ All pass?
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Concurrency Control                        â”‚
â”‚              (Max 5 concurrent LLM calls per user)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Priority Queue                             â”‚
â”‚              (Paid users > Free users)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LLM Provider                            â”‚
â”‚              (Also has its own rate limits!)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Handling Provider Limits

```
Your app has limits, but so does OpenAI:

OpenAI tier limits (example):
â”œâ”€â”€ Tier 1: 60K TPM, $100/month
â”œâ”€â”€ Tier 2: 80K TPM, $500/month
â”œâ”€â”€ Tier 4: 300K TPM, $5000/month
â””â”€â”€ Tier 5: 10M TPM

Strategy:
â”œâ”€â”€ Pool across multiple API keys
â”œâ”€â”€ Distribute load across providers
â”œâ”€â”€ Queue requests during burst
â””â”€â”€ Prefer cached responses
```

### Multi-Provider Fallback

```python
providers = ["openai", "anthropic", "azure_openai"]

def call_llm(prompt):
    for provider in providers:
        if rate_limiter.can_use(provider):
            try:
                return provider.generate(prompt)
            except RateLimitError:
                rate_limiter.mark_limited(provider, duration=60)
                continue
    
    # All providers limited
    return queue_for_later(prompt)
```

---

## 7. Trade-offs

| Decision | Option A | Option B | When to Choose A |
|----------|----------|----------|------------------|
| **Rejection** | 429 error | Queue request | User can retry |
| **Granularity** | Per-user | Per-org | B2C application |
| **Enforcement** | Hard limit | Soft (degrade) | Cost protection |
| **Measurement** | Input tokens only | Input + output | Accurate cost |

---

## 8. Interview Explanation

> "For an LLM-powered application, I'd implement multi-dimensional rate limiting.
>
> First, request rate: 60 requests per minute per user prevents script abuse. Second, token rate: 100K tokens per minute limits throughput. Third, daily cost budget: $10/day per free user, $50/day per paid user.
>
> Before each request, I estimate token count from prompt length and check all limits. If any exceeds, return 429 with Retry-After header.
>
> For handling provider limits, I'd use a token bucket shared across all users, sized to 80% of our OpenAI tier limit. During bursts, requests queue with priority based on user tier.
>
> Multi-provider fallback gives resilience: if OpenAI is rate-limited, route to Anthropic or Azure OpenAI.
>
> The trade-off is UX for heavy users. They may hit limits and need to wait. We'd show clear messaging and upgrade paths."

---

## 9. Common Mistakes

- **Only count requests**: "60 req/min" â†’ One 100K token request bypasses
- **Ignore output tokens**: "Limit input" â†’ Output can be 10x input
- **No cost protection**: "No spending limit" â†’ $10K bill surprise
- **Single provider**: "OpenAI only" â†’ One outage = full outage
- **No priority**: "FIFO queue" â†’ Free users starve paid users

---

## 10. Senior-Level Phrases

- "Multi-dimensional limits: requests, tokens, and cost budget."
- "Token bucket allows bursts while enforcing sustained rate."
- "Provider-level pooling with fallback ensures resilience."
- "Estimate tokens before the call to fail fast on limits."
