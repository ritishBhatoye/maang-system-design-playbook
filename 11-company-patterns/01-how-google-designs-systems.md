# ğŸ” How Google Designs Systems (2026 MAANG Edition)

## 1. Why This Matters in Interviews

Understanding Google's architectural philosophy helps you think like a **Staff+ engineer**. Google invented many foundational distributed systems concepts.

---

## 2. Google's Core Principles

### 1. Design for Scale from Day One

```
Google's mindset:
â”œâ”€â”€ Billions of queries per day
â”œâ”€â”€ Petabytes of data
â”œâ”€â”€ Millions of servers
â””â”€â”€ Global, always-on

"If it doesn't scale to 1000x, we need a different design."
```

### 2. Embrace Distributed Systems

```
Core assumption: Machines fail
â”œâ”€â”€ Single server MTBF: ~3 years
â”œâ”€â”€ At 1M servers: ~1 server fails per minute
â””â”€â”€ Design for failure, not prevention
```

### 3. Data-Driven Everything

```
Every decision backed by data:
â”œâ”€â”€ A/B testing at massive scale
â”œâ”€â”€ Metrics, metrics, metrics
â””â”€â”€ SLOs drive engineering priorities
```

---

## 3. Key Architectural Patterns

### Bigtable Pattern (Wide-Column Store)

```
Schema-less, sorted key-value:

Row Key          | Column Family: info | Column Family: metrics
-----------------+--------------------+------------------------
com.google.www   | content: <html>... | visits: 1M, rank: 1
com.youtube.www  | content: <html>... | visits: 500M, rank: 2

Use case: When you need sorted scans and column families
```

### Spanner Pattern (Globally Distributed SQL)

```
Strong consistency across regions via TrueTime:

Region US â†’ Timestamp T1 (atomic clock)
Region EU â†’ Timestamp T2 (atomic clock)
â””â”€â”€ TrueTime bounds uncertainty to < 7ms

Use case: Global transactions, financial data
```

### MapReduce â†’ Dataflow (Stream + Batch)

```
Unified model for batch and streaming:

Pipeline:
â”œâ”€â”€ Read from source (batch or streaming)
â”œâ”€â”€ Transform (map, filter, aggregate)
â”œâ”€â”€ Group by key
â”œâ”€â”€ Window (time-based for streaming)
â””â”€â”€ Write to sink

Use case: ETL, analytics, ML feature pipelines
```

---

## 4. Google's Infrastructure Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Applications                          â”‚
â”‚              (Gmail, Search, YouTube)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Serverless/Managed                     â”‚
â”‚          (Cloud Functions, Cloud Run, GKE)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Data Layer                           â”‚
â”‚     (Bigtable, Spanner, BigQuery, Firestore)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   Networking/Infra                       â”‚
â”‚        (Borg, Colossus, Stubby/gRPC, Chubby)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     Hardware                             â”‚
â”‚      (Custom servers, TPUs, Global fiber network)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Key Systems to Know

### Borg (Container Orchestration)

```
Kubernetes' ancestor:
â”œâ”€â”€ Declarative job specification
â”œâ”€â”€ Resource bin-packing
â”œâ”€â”€ Automatic scheduling and rescheduling
â””â”€â”€ Cell-based deployment (cluster of 10K machines)
```

### Chubby (Distributed Lock Service)

```
Coordination primitive:
â”œâ”€â”€ Distributed locking
â”œâ”€â”€ Leader election
â”œâ”€â”€ Configuration storage
â””â”€â”€ Service discovery

Open-source equivalent: ZooKeeper
```

### Colossus (Distributed File System)

```
GFS v2:
â”œâ”€â”€ Exabyte-scale storage
â”œâ”€â”€ Automatic replication
â”œâ”€â”€ Single namespace across data centers
â””â”€â”€ Optimized for large sequential I/O
```

---

## 6. Trade-offs Google Makes

| Decision | Google's Choice | Trade-off Accepted |
|----------|-----------------|-------------------|
| **Consistency** | Strong (Spanner) | Higher latency |
| **Storage** | Custom (Bigtable) | Operational complexity |
| **Compute** | Preemptible VMs | Jobs can be killed |
| **Networking** | Private backbone | Massive infrastructure cost |

---

## 7. Interview Application

> "Google's approach emphasizes horizontal scaling with commodity hardware, accepting failures as normal. For a global system, I'd consider the Spanner pattern: use TrueTime-like mechanisms for global consensus if strong consistency is required.
>
> For high-throughput data, the Bigtable model of sorted key-value with column families works wellâ€”similar to HBase or Cassandra.
>
> I'd design with SLOs in mind: define the error budget, then architect to meet it rather than over-engineering for zero failures."

---

## 8. Senior-Level Phrases

- "I'd apply the Bigtable model: sorted keys with column families for flexible schemas."
- "Spanner's TrueTime gives strong consistency globally at the cost of 10-20ms latency."
- "Like Borg, I'd use declarative specifications and let the scheduler handle placement."
- "Following SRE principles, I'd define an error budget before architecting."
