# ðŸ“Š Scaling Analytics Systems (2026 MAANG Edition)

## 1. Why This Matters in Interviews

Analytics systems have **fundamentally different access patterns** than OLTP. Interviewers test this to see if you can design for large scans, aggregations, and batch processing.

---

## 2. Problem Interviewers Are Testing

- Do you understand OLAP vs OLTP differences?
- Can you design for analytical queries (aggregations, scans)?
- Do you know when to use columnar storage?

---

## 3. Key Concepts

### OLTP vs OLAP

| Characteristic | OLTP | OLAP |
|----------------|------|------|
| **Query pattern** | Point lookups | Full scans, aggregations |
| **Data size** | Row at a time | Billions of rows |
| **Latency** | < 10ms | Seconds to minutes |
| **Freshness** | Real-time | Minutes to hours delayed |
| **Storage** | Row-oriented | Column-oriented |

### Analytics Query Characteristics

```
Typical OLAP query:
SELECT date, country, SUM(revenue), COUNT(DISTINCT user_id)
FROM events
WHERE date BETWEEN '2024-01-01' AND '2024-12-31'
GROUP BY date, country
ORDER BY revenue DESC

Scans: 10 billion rows
Returns: 1,000 rows
Time: 10-60 seconds
```

---

## 4. Storage Patterns

### Columnar Storage

```
Row Storage (OLTP):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ID   â”‚ Name â”‚ Country â”‚Revenue â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1    â”‚ John â”‚   US    â”‚  100   â”‚
â”‚   2    â”‚ Jane â”‚   UK    â”‚  200   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Columnar Storage (OLAP):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ID   â”‚  â”‚ Name â”‚  â”‚ Country â”‚  â”‚Revenue â”‚
â”‚   1    â”‚  â”‚ John â”‚  â”‚   US    â”‚  â”‚  100   â”‚
â”‚   2    â”‚  â”‚ Jane â”‚  â”‚   UK    â”‚  â”‚  200   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefit: Read only columns you need
SUM(revenue) â†’ Only scan Revenue column
```

### Partitioning for Analytics

```
Events table partitioned by date:

events/
â”œâ”€â”€ year=2024/
â”‚   â”œâ”€â”€ month=01/
â”‚   â”‚   â”œâ”€â”€ day=01/
â”‚   â”‚   â”œâ”€â”€ day=02/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ month=02/
â””â”€â”€ year=2025/

WHERE date = '2024-01-15'
â†’ Only scans 1 partition (1/365th of data)
```

---

## 5. Architecture Pattern

```
                     Data Sources
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚   Kafka     â”‚
                   â”‚ (streaming) â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼           â–¼           â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Flink  â”‚ â”‚  Spark  â”‚ â”‚  S3/GCS  â”‚
        â”‚(real-time)â”‚ â”‚ (batch) â”‚ â”‚  (lake)  â”‚
        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
             â”‚            â”‚           â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ Data        â”‚
                   â”‚ Warehouse   â”‚
                   â”‚ (Snowflake) â”‚
                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚ BI / Query  â”‚
                   â”‚   Tools     â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Aggregation Strategies

### Pre-Aggregation (Rollups)

```
Raw events: 10 billion rows
â”œâ”€â”€ Per-minute rollup: 525,600 rows/year
â”œâ”€â”€ Per-hour rollup: 8,760 rows/year
â””â”€â”€ Daily rollup: 365 rows/year

Query "daily revenue for 2024":
â”œâ”€â”€ Without rollup: Scan 10B rows â†’ 60 seconds
â””â”€â”€ With rollup: Scan 365 rows â†’ 10ms
```

### Materialized Views

```sql
-- Pre-compute common aggregations
CREATE MATERIALIZED VIEW daily_revenue AS
SELECT 
    date,
    country,
    SUM(revenue) as total_revenue,
    COUNT(DISTINCT user_id) as unique_users
FROM events
GROUP BY date, country;

-- Refresh periodically
REFRESH MATERIALIZED VIEW daily_revenue;
```

---

## 7. Trade-offs

| Decision | Option A | Option B | When to Choose A |
|----------|----------|----------|------------------|
| **Freshness** | Real-time | Batch (hourly) | Alerting, monitoring |
| **Storage** | Columnar | Row-based | Analytical queries |
| **Query speed** | Pre-aggregated | Raw data | Known query patterns |
| **Cost** | On-demand (Athena) | Provisioned (Redshift) | Variable query volume |

---

## 8. Interview Explanation

> "For an analytics system processing 10 billion events per day, I'd design a Lambda architecture combining batch and stream processing.
>
> Events flow through Kafka into two paths: Flink for real-time aggregations (last 1 hour) and Spark batch jobs for historical data loaded into a data warehouse like Snowflake.
>
> For storage, columnar format (Parquet) partitioned by date. A query for 'revenue by country for 2024' only scans the relevant columns and partitionsâ€”reducing I/O by 100x compared to row storage.
>
> For common queries, I'd pre-aggregate with materialized views: daily rollups, weekly rollups. The dashboard query hits the 365-row rollup table instead of 10 billion raw events.
>
> The trade-off is freshness. Batch data is 1 hour delayed, real-time data is within seconds but less complete. We'd use real-time for monitoring/alerting and batch for reporting."

---

## 9. Common Mistakes

- **OLTP database for analytics**: "Just use PostgreSQL" â†’ Scans too slow
- **No partitioning**: "One big table" â†’ Full scan every query
- **Always real-time**: "Stream everything" â†’ Expensive, complex
- **Ignoring pre-aggregation**: "Query raw data" â†’ Slow dashboards

---

## 10. Senior-Level Phrases

- "Columnar storage reduces I/O by only scanning needed columns."
- "Date partitioning with predicate pushdown eliminates 99% of scans."
- "Pre-aggregated rollups trade storage for query performance."
- "Lambda architecture balances real-time freshness with batch completeness."
