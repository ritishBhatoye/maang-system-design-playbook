# ðŸ’¾ Prompt Caching (2026 MAANG Edition)

## 1. Why This Matters in Interviews

Prompt caching can reduce LLM costs by **10x** and latency by **50x**. In 2026, this is essential for any production AI system.

---

## 2. Problem Interviewers Are Testing

- Do you understand the cost/latency of LLM calls?
- Can you identify cacheable patterns in prompts?
- Do you know exact vs semantic caching strategies?
- Can you design cache invalidation for AI systems?

---

## 3. Key Concepts

### Why Cache Prompts?

```
LLM costs:
â”œâ”€â”€ GPT-4o: $2.50/1M input tokens, $10/1M output tokens
â”œâ”€â”€ Claude Sonnet: $3/1M input, $15/1M output
â””â”€â”€ At scale: Millions of dollars/month

LLM latency:
â”œâ”€â”€ GPT-4: 5-30 seconds for complex responses
â”œâ”€â”€ First token latency: 500ms-2s
â””â”€â”€ User waiting = bad UX

Caching value:
â”œâ”€â”€ Same question? Return cached answer
â”œâ”€â”€ Cost: $0 (after first call)
â”œâ”€â”€ Latency: 10ms (cache lookup)
```

### Types of Caching

| Type | Match | Hit Rate | Complexity |
|------|-------|----------|------------|
| **Exact** | Identical prompt | 10-30% | Low |
| **Semantic** | Similar meaning | 40-70% | Medium |
| **Prefix** | Same system prompt | 90%+ | Low |
| **KV Cache** | Same conversation | 100% | High |

---

## 4. Caching Strategies

### 1. Exact Match Caching

```
Simple hash-based lookup:

cache_key = hash(prompt)
if cache.exists(cache_key):
    return cache.get(cache_key)
else:
    response = llm.generate(prompt)
    cache.set(cache_key, response, ttl=3600)
    return response

Pros: Simple, guaranteed accuracy
Cons: Low hit rate, minor variations miss
```

### 2. Semantic Caching

```
Embedding-based similarity:

query_embedding = embed(prompt)
similar = vector_db.search(query_embedding, threshold=0.95)

if similar and similar.score > 0.95:
    return similar.cached_response
else:
    response = llm.generate(prompt)
    vector_db.insert(query_embedding, response)
    return response

Pros: High hit rate
Cons: Approximate matches may differ
```

### 3. Prefix Caching (Provider-Level)

```
Many prompts share the same prefix:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SYSTEM: You are a helpful assistant for ACME... â”‚ â† Shared prefix
â”‚ (1000 tokens of context)                        â”‚ â† Cached at provider
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ USER: What is your return policy?               â”‚ â† Unique per request
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OpenAI/Anthropic cache the prefix â†’ 50% token cost reduction
```

### 4. KV Cache (Conversation)

```
Multi-turn conversation:

Turn 1: [System] [User1] [Assistant1]
Turn 2: [System] [User1] [Assistant1] [User2] [Assistant2] â† Cache previous KV

Provider caches attention KV from previous turns
â†’ Only compute new tokens
â†’ Faster responses in conversations
```

---

## 5. Architecture Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Request                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Exact Cache Check                        â”‚
â”‚              hash(prompt) â†’ Redis lookup                     â”‚
â”‚                                                              â”‚
â”‚              HIT? â†’ Return cached response                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ MISS
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Semantic Cache Check                       â”‚
â”‚           embed(prompt) â†’ Vector DB similarity               â”‚
â”‚                                                              â”‚
â”‚              SIMILAR? â†’ Return cached response               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ MISS
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        LLM Call                              â”‚
â”‚                                                              â”‚
â”‚              Generate â†’ Store in both caches                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 6. Cache Invalidation

```
When to invalidate:
â”œâ”€â”€ Knowledge update (new docs indexed)
â”œâ”€â”€ Time-based (TTL expired)
â”œâ”€â”€ User feedback ("bad answer" signal)
â”œâ”€â”€ Model update (new model version)
â””â”€â”€ Context change (different user permissions)

Strategy:
â”œâ”€â”€ TTL: Set reasonable expiry (1h - 24h)
â”œâ”€â”€ Version key: cache_key + model_version
â”œâ”€â”€ Segment by user: Don't share across tenants
â””â”€â”€ Soft invalidation: Flag stale, regenerate async
```

---

## 7. Trade-offs

| Decision | Option A | Option B | When to Choose A |
|----------|----------|----------|------------------|
| **Matching** | Exact | Semantic | Deterministic needed |
| **Scope** | Global | Per-user | Privacy concerns |
| **TTL** | Short (1h) | Long (24h) | Frequently changing data |
| **Storage** | Redis | Vector DB | Exact match sufficient |

---

## 8. Interview Explanation

> "For an LLM-powered Q&A system with 10K queries/day, I'd implement tiered prompt caching.
>
> First layer: exact match caching in Redis. Hash the normalized prompt (lowercase, trimmed) and check cache. This catches repeated questions with 20-30% hit rate.
>
> Second layer: semantic caching. Embed the prompt and search the vector DB for similar past queries (cosine similarity > 0.95). If found, return the cached response. This catches paraphrased questions, pushing hit rate to 50-60%.
>
> For the remaining 40%, we call the LLM and store the response in both caches.
>
> Cache invalidation uses TTL (24 hours for FAQ-style content) plus manual invalidation when underlying docs change. We version the cache key by model version so model updates don't serve stale responses.
>
> The trade-off is freshness for cost. With 60% cache hit rate, we reduce LLM costs by 2.5x and average latency by 10x."

---

## 9. Common Mistakes

- **No caching**: "Every query hits LLM" â†’ Expensive at scale
- **Too aggressive semantic**: "0.8 similarity threshold" â†’ Wrong answers
- **No invalidation**: "Cache forever" â†’ Stale/wrong responses
- **Shared across users**: "One global cache" â†’ Permission leaks
- **Ignoring prompt structure**: "Hash entire prompt" â†’ Miss prefix sharing

---

## 10. Senior-Level Phrases

- "Tiered caching: exact match first, semantic similarity second."
- "0.95+ similarity threshold prevents false positive cache hits."
- "Prefix caching at the provider level reduces token costs by 50%."
- "Cache versioned by model ID to handle model updates cleanly."
