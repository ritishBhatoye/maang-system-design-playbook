# ðŸŽ¬ How Netflix Handles Streaming (2026 MAANG Edition)

## 1. Why This Matters in Interviews

Netflix's streaming architecture demonstrates **CDN at global scale** and is a perfect case study for content delivery system design.

---

## 2. Netflix's Scale

```
The challenge:
â”œâ”€â”€ 200+ million subscribers
â”œâ”€â”€ 100+ million hours watched daily
â”œâ”€â”€ Peak: 20%+ of global internet traffic
â”œâ”€â”€ 190+ countries
â””â”€â”€ 4K HDR = 25 Mbps per stream

Key constraint: Cannot rely on public internet
```

---

## 3. Two-Cloud Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CONTROL PLANE (AWS)                       â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ User    â”‚  â”‚ Catalog â”‚  â”‚ Billing â”‚  â”‚ Recommendation â”‚ â”‚â”‚
â”‚  â”‚ Service â”‚  â”‚ Service â”‚  â”‚ Service â”‚  â”‚    Engine       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                              â”‚
â”‚               Everything EXCEPT video bytes                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 DATA PLANE (Open Connect CDN)               â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚   OCA     â”‚  â”‚   OCA     â”‚  â”‚   OCA     â”‚               â”‚
â”‚  â”‚(US-East)  â”‚  â”‚(EU-West)  â”‚  â”‚(Tokyo)    â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                              â”‚
â”‚               18,000+ OCAs in 175 countries                 â”‚
â”‚               100% of video traffic                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4. Open Connect Architecture

### Open Connect Appliances (OCAs)

```
Custom servers deployed inside ISPs:
â”œâ”€â”€ Flash storage: ~100-200TB per OCA
â”œâ”€â”€ 100 Gbps network throughput
â”œâ”€â”€ Strategically placed in ISP networks
â”œâ”€â”€ FREE to ISPs (Netflix pays for hardware)
â””â”€â”€ Result: Netflix traffic stays local

ISP benefits:
â”œâ”€â”€ Reduced upstream bandwidth costs
â”œâ”€â”€ Better user experience
â””â”€â”€ Less network congestion
```

### Content Flow

```
Content Creation â†’ Encoding â†’ Distribution â†’ Playback

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Master File â”‚
â”‚ (4K, 100GB) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Encode (AWS)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 50 Versions â”‚
â”‚ (codec,     â”‚
â”‚ bitrate,    â”‚
â”‚ resolution) â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Distribute (Off-peak)
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Open Connect OCAs              â”‚
â”‚  (content pushed during 2 AM - 2 PM local)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 5. Adaptive Streaming

```
Playback adapts to network conditions:

Good connection:    4K HDR @ 25 Mbps
Moderate:           1080p @ 5 Mbps
Poor:               480p @ 1.5 Mbps

Client algorithm:
1. Monitor buffer fill rate
2. Estimate available bandwidth
3. Request appropriate quality chunk
4. Seamlessly switch mid-stream
```

### Chunk-Based Delivery

```
Video broken into chunks:
â”œâ”€â”€ Each chunk: 2-4 seconds
â”œâ”€â”€ Pre-encoded at multiple bitrates
â”œâ”€â”€ Client fetches chunk by chunk
â””â”€â”€ Can switch quality per chunk

Example request:
GET /video/123/chunk-42-1080p.mp4
```

---

## 6. Resilience Patterns

### Chaos Engineering (Chaos Monkey)

```
Netflix invented chaos engineering:
â”œâ”€â”€ Randomly kill instances in production
â”œâ”€â”€ Test DR during business hours
â”œâ”€â”€ Build confidence in resilience
â””â”€â”€ "If you want to find bugs, break things"

Simian Army:
â”œâ”€â”€ Chaos Monkey: Kill instances
â”œâ”€â”€ Chaos Kong: Kill entire regions
â”œâ”€â”€ Latency Monkey: Inject delays
â””â”€â”€ Conformity Monkey: Find non-conforming instances
```

### Hystrix (Circuit Breaker)

```
Every service call wrapped with:
â”œâ”€â”€ Timeout (prevent indefinite waits)
â”œâ”€â”€ Circuit breaker (stop calling failing service)
â”œâ”€â”€ Fallback (degrade gracefully)
â”œâ”€â”€ Bulkhead (isolate thread pools)
```

---

## 7. Trade-offs Netflix Makes

| Decision | Netflix's Choice | Trade-off |
|----------|-----------------|-----------|
| **CDN** | Own infrastructure | Massive CAPEX, but control |
| **Encoding** | Pre-encode 50 versions | Storage cost, but fast playback |
| **Reliability** | Break in production | Risk, but builds resilience |
| **Control plane** | All on AWS | Single cloud dependency |

---

## 8. Interview Application

> "For a streaming platform at scale, I'd follow Netflix's two-cloud pattern. The control plane (user service, catalog, recommendations) runs on a public cloud like AWS for flexibility.
>
> For video delivery, I'd build or partner for a CDN with edge caches close to users. The key insight is that cached video content should be served from inside ISP networks, not across the public internet.
>
> Content is pre-encoded into multiple bitrates and resolutions. The player uses adaptive streaming: monitoring available bandwidth and switching quality seamlessly.
>
> For resilience, I'd implement circuit breakers on all service calls and practice chaos engineeringâ€”randomly killing instances to find weaknesses before they cause outages.
>
> The trade-off is infrastructure investment. Building owned CDN is expensive but provides control over quality at scale."

---

## 9. Senior-Level Phrases

- "I'd separate control plane (AWS) from data plane (CDN) for independent scaling."
- "Edge caches inside ISP networks eliminate public internet bottlenecks."
- "Adaptive bitrate streaming adjusts quality per chunk based on bandwidth."
- "Chaos engineering builds confidence in resilienceâ€”break things intentionally."
